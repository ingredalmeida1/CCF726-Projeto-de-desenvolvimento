{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DTA5sZmXuH_"
   },
   "source": [
    "## **Edge-Supervised GraphSAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meITCimKYN22"
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y jax jaxlib\n",
    "%pip install -U \"tensorflow==2.17.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1761515059062,
     "user": {
      "displayName": "Ingred Almeida",
      "userId": "12962108153732502929"
     },
     "user_tz": 180
    },
    "id": "nVggfuyWYhVh",
    "outputId": "d2f1d879-1474-4f86-99d4-35b6f7a9e97c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf, sys\n",
    "print(\"TF\", tf.__version__, \"Python\", sys.version)\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1761515180391,
     "user": {
      "displayName": "Ingred Almeida",
      "userId": "12962108153732502929"
     },
     "user_tz": 180
    },
    "id": "QVSMzHWzZE-J"
   },
   "outputs": [],
   "source": [
    "# ==== SHIM TF1->TF2 ====\n",
    "import types, tensorflow as tf\n",
    "tf1 = tf.compat.v1\n",
    "tf1.disable_eager_execution()\n",
    "\n",
    "tf.app = tf1.app\n",
    "tf.variable_scope = tf1.variable_scope\n",
    "tf.get_variable   = tf1.get_variable\n",
    "tf.summary        = tf1.summary\n",
    "tf.train          = tf1.train\n",
    "tf.set_random_seed= tf1.set_random_seed\n",
    "tf.global_variables_initializer = tf1.global_variables_initializer\n",
    "tf.reset_default_graph         = tf1.reset_default_graph\n",
    "\n",
    "tf.random_uniform = tf1.random_uniform\n",
    "tf.random_shuffle = tf1.random_shuffle\n",
    "tf.nn.dropout     = tf1.nn.dropout\n",
    "tf.placeholder    = tf1.placeholder\n",
    "tf.Session        = tf1.Session\n",
    "\n",
    "def _xavier_init():\n",
    "    return tf1.keras.initializers.glorot_uniform()\n",
    "\n",
    "tf.contrib = types.SimpleNamespace(\n",
    "    layers = types.SimpleNamespace(\n",
    "        xavier_initializer = _xavier_init,\n",
    "        l2_regularizer    = tf.keras.regularizers.l2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1761516110104,
     "user": {
      "displayName": "Ingred Almeida",
      "userId": "12962108153732502929"
     },
     "user_tz": 180
    },
    "id": "NkADuO6JaMex"
   },
   "outputs": [],
   "source": [
    "import os, json, time, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from networkx.readwrite import json_graph\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve, precision_score, recall_score, accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eecn-2_QZR0Y"
   },
   "source": [
    "### **GraphSage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDQpUIAfZt_E"
   },
   "source": [
    "#### Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761515190983,
     "user": {
      "displayName": "Ingred Almeida",
      "userId": "12962108153732502929"
     },
     "user_tz": 180
    },
    "id": "Tj2dWnvKZvpS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# DISCLAIMER:\n",
    "# Parts of this code file are derived from\n",
    "# https://github.com/tkipf/gcn\n",
    "# which is under an identical MIT license as GraphSAGE\n",
    "\n",
    "def uniform(shape, scale=0.05, name=None):\n",
    "    \"\"\"Uniform init.\"\"\"\n",
    "    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    \"\"\"All ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_zCJocxZqp8"
   },
   "source": [
    "#### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1761515192097,
     "user": {
      "displayName": "Ingred Almeida",
      "userId": "12962108153732502929"
     },
     "user_tz": 180
    },
    "id": "vHWbjaVqZxUE"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# DISCLAIMER:\n",
    "# Boilerplate parts of this code file were originally forked from\n",
    "# https://github.com/tkipf/gcn\n",
    "# which itself was very inspired by the keras package\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging', 'model_size'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.,\n",
    "                 act=tf.nn.relu, placeholders=None, bias=True, featureless=False,\n",
    "                 sparse_inputs=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.act = act\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        if sparse_inputs:\n",
    "            self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = tf.get_variable('weights', shape=(input_dim, output_dim),\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         regularizer=tf.contrib.layers.l2_regularizer(FLAGS.weight_decay))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = tf.matmul(x, self.vars['weights'])\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa5nZHE7ZlCU"
   },
   "source": [
    "#### Aggregators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1761515425247,
     "user": {
      "displayName": "Ingred Almeida",
      "userId": "12962108153732502929"
     },
     "user_tz": 180
    },
    "id": "2ICfrOFEZOB1"
   },
   "outputs": [],
   "source": [
    "class MeanAggregator(Layer):\n",
    "    \"\"\"\n",
    "    Aggregates via mean followed by matmul and non-linearity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu,\n",
    "            name=None, concat=False, **kwargs):\n",
    "        super(MeanAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([neigh_input_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "\n",
    "        neigh_vecs = tf.nn.dropout(neigh_vecs, 1-self.dropout)\n",
    "        self_vecs = tf.nn.dropout(self_vecs, 1-self.dropout)\n",
    "        neigh_means = tf.reduce_mean(neigh_vecs, axis=1)\n",
    "\n",
    "        # [nodes] x [out_dim]\n",
    "        from_neighs = tf.matmul(neigh_means, self.vars['neigh_weights'])\n",
    "\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "\n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "class GCNAggregator(Layer):\n",
    "    \"\"\"\n",
    "    Aggregates via mean followed by matmul and non-linearity.\n",
    "    Same matmul parameters are used self vector and neighbor vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(GCNAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['weights'] = glorot([neigh_input_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "\n",
    "        neigh_vecs = tf.nn.dropout(neigh_vecs, 1-self.dropout)\n",
    "        self_vecs = tf.nn.dropout(self_vecs, 1-self.dropout)\n",
    "        means = tf.reduce_mean(tf.concat([neigh_vecs,\n",
    "            tf.expand_dims(self_vecs, axis=1)], axis=1), axis=1)\n",
    "\n",
    "        # [nodes] x [out_dim]\n",
    "        output = tf.matmul(means, self.vars['weights'])\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class MaxPoolingAggregator(Layer):\n",
    "    \"\"\" Aggregates via max-pooling over MLP functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(MaxPoolingAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim = self.hidden_dim = 512\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim = self.hidden_dim = 1024\n",
    "\n",
    "        self.mlp_layers = []\n",
    "        self.mlp_layers.append(Dense(input_dim=neigh_input_dim,\n",
    "                                 output_dim=hidden_dim,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=dropout,\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "\n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "        neigh_h = neigh_vecs\n",
    "\n",
    "        dims = tf.shape(neigh_h)\n",
    "        batch_size = dims[0]\n",
    "        num_neighbors = dims[1]\n",
    "        # [nodes * sampled neighbors] x [hidden_dim]\n",
    "        h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self.neigh_input_dim))\n",
    "\n",
    "        for l in self.mlp_layers:\n",
    "            h_reshaped = l(h_reshaped)\n",
    "        neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self.hidden_dim))\n",
    "        neigh_h = tf.reduce_max(neigh_h, axis=1)\n",
    "\n",
    "        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "\n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "class MeanPoolingAggregator(Layer):\n",
    "    \"\"\" Aggregates via mean-pooling over MLP functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(MeanPoolingAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim = self.hidden_dim = 512\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim = self.hidden_dim = 1024\n",
    "\n",
    "        self.mlp_layers = []\n",
    "        self.mlp_layers.append(Dense(input_dim=neigh_input_dim,\n",
    "                                 output_dim=hidden_dim,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=dropout,\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "\n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "        neigh_h = neigh_vecs\n",
    "\n",
    "        dims = tf.shape(neigh_h)\n",
    "        batch_size = dims[0]\n",
    "        num_neighbors = dims[1]\n",
    "        # [nodes * sampled neighbors] x [hidden_dim]\n",
    "        h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self.neigh_input_dim))\n",
    "\n",
    "        for l in self.mlp_layers:\n",
    "            h_reshaped = l(h_reshaped)\n",
    "        neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self.hidden_dim))\n",
    "        neigh_h = tf.reduce_mean(neigh_h, axis=1)\n",
    "\n",
    "        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "\n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class TwoMaxLayerPoolingAggregator(Layer):\n",
    "    \"\"\" Aggregates via pooling over two MLP functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(TwoMaxLayerPoolingAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim_1 = self.hidden_dim_1 = 512\n",
    "            hidden_dim_2 = self.hidden_dim_2 = 256\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim_1 = self.hidden_dim_1 = 1024\n",
    "            hidden_dim_2 = self.hidden_dim_2 = 512\n",
    "\n",
    "        self.mlp_layers = []\n",
    "        self.mlp_layers.append(Dense(input_dim=neigh_input_dim,\n",
    "                                 output_dim=hidden_dim_1,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=dropout,\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "        self.mlp_layers.append(Dense(input_dim=hidden_dim_1,\n",
    "                                 output_dim=hidden_dim_2,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=dropout,\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim_2, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "\n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "        neigh_h = neigh_vecs\n",
    "\n",
    "        dims = tf.shape(neigh_h)\n",
    "        batch_size = dims[0]\n",
    "        num_neighbors = dims[1]\n",
    "        # [nodes * sampled neighbors] x [hidden_dim]\n",
    "        h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self.neigh_input_dim))\n",
    "\n",
    "        for l in self.mlp_layers:\n",
    "            h_reshaped = l(h_reshaped)\n",
    "        neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self.hidden_dim_2))\n",
    "        neigh_h = tf.reduce_max(neigh_h, axis=1)\n",
    "\n",
    "        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "\n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "class SeqAggregator(Layer):\n",
    "    \"\"\" Aggregates via a standard LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None,  concat=False, **kwargs):\n",
    "        super(SeqAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim = self.hidden_dim = 128\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim = self.hidden_dim = 256\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "\n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "        # self.cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_dim)\n",
    "        try:\n",
    "            self.cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.hidden_dim)\n",
    "            self._use_dynamic_rnn = True\n",
    "        except Exception:\n",
    "            self.cell = tf.keras.layers.LSTMCell(self.hidden_dim)\n",
    "            self._use_dynamic_rnn = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "      self_vecs, neigh_vecs = inputs\n",
    "\n",
    "      used   = tf.sign(tf.reduce_max(tf.abs(neigh_vecs), axis=2))\n",
    "      lengths = tf.cast(tf.maximum(tf.reduce_sum(used, axis=1), 1.0), tf.int32)\n",
    "      batch_size = tf.shape(neigh_vecs)[0]\n",
    "\n",
    "      if getattr(self, \"_use_dynamic_rnn\", False):\n",
    "          initial_state = self.cell.zero_state(batch_size, tf.float32)\n",
    "          with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE):\n",
    "              rnn_outputs, _ = tf.compat.v1.nn.dynamic_rnn(\n",
    "                  self.cell,\n",
    "                  neigh_vecs,\n",
    "                  initial_state=initial_state,\n",
    "                  dtype=tf.float32,\n",
    "                  time_major=False,\n",
    "                  sequence_length=lengths\n",
    "              )\n",
    "      else:\n",
    "          mask_bool = tf.cast(used, tf.bool)\n",
    "          rnn_layer = tf.keras.layers.RNN(\n",
    "              self.cell, return_sequences=True, return_state=False, name=self.name\n",
    "          )\n",
    "          rnn_outputs = rnn_layer(neigh_vecs, mask=mask_bool, training=False)\n",
    "\n",
    "      max_len  = tf.shape(rnn_outputs)[1]\n",
    "      out_size = tf.shape(rnn_outputs)[2]\n",
    "      idx = tf.range(batch_size) * max_len + (lengths - 1)\n",
    "      flat = tf.reshape(rnn_outputs, [-1, out_size])\n",
    "      neigh_h = tf.gather(flat, idx)\n",
    "\n",
    "      from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "      from_self   = tf.matmul(self_vecs, self.vars['self_weights'])\n",
    "\n",
    "      if not self.concat:\n",
    "          output = tf.add_n([from_self, from_neighs])\n",
    "      else:\n",
    "          output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "      if self.bias:\n",
    "          output += self.vars['bias']\n",
    "\n",
    "      return self.act(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlavrXa8Z-A6"
   },
   "source": [
    "#### Neighbor Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1761515427084,
     "user": {
      "displayName": "Ingred Almeida",
      "userId": "12962108153732502929"
     },
     "user_tz": 180
    },
    "id": "6bkKm-7IaDUH"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\"\"\"\n",
    "Classes that are used to sample node neighborhoods\n",
    "\"\"\"\n",
    "\n",
    "class UniformNeighborSampler(Layer):\n",
    "    \"\"\"\n",
    "    Uniformly samples neighbors.\n",
    "    Assumes that adj lists are padded with random re-sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_info, **kwargs):\n",
    "        super(UniformNeighborSampler, self).__init__(**kwargs)\n",
    "        self.adj_info = adj_info\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        ids, num_samples = inputs\n",
    "        adj_lists = tf.nn.embedding_lookup(self.adj_info, ids)\n",
    "        adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(adj_lists)))\n",
    "        adj_lists = tf.slice(adj_lists, [0,0], [-1, num_samples])\n",
    "        return adj_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17tbkHw9ZVpj"
   },
   "source": [
    "### **Edge Supervised GraphSage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761513897896,
     "user": {
      "displayName": "Ingred Almeida",
      "userId": "12962108153732502929"
     },
     "user_tz": 180
    },
    "id": "W0W5p-5haKgl"
   },
   "outputs": [],
   "source": [
    "tf1 = tf.compat.v1; tf1.disable_eager_execution()\n",
    "xavier_init = tf1.keras.initializers.glorot_uniform()\n",
    "zeros_init  = tf1.zeros_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q1VjSYbahmG"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "DATA_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33VT8crwaRfS"
   },
   "outputs": [],
   "source": [
    "PROC_DIR   = os.path.join(DATA_DIR, 'processed')\n",
    "OUT_PREFIX = os.path.join(PROC_DIR, 'graphsage_edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Gctj9fvajfO"
   },
   "outputs": [],
   "source": [
    "AGGREGATOR = \"mean\"     # \"mean\",\"gcn\",\"maxpool\",\"meanpool\",\"seq\"\n",
    "DIM_1, DIM_2 = 128, 128\n",
    "SAMPLES_1, SAMPLES_2 = 25, 10\n",
    "BATCH_SIZE = 1024\n",
    "DROPOUT = 0.1\n",
    "LR = 1e-3\n",
    "EPOCHS = 2\n",
    "MAX_DEGREE = 128\n",
    "CP_EMB_DIM = 8\n",
    "SEED = 123\n",
    "np.random.seed(SEED); tf1.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oJbG4vRamRn"
   },
   "outputs": [],
   "source": [
    "def load_graph_base(prefix):\n",
    "    G_data = json.load(open(prefix + \"-G.json\"))\n",
    "    G = json_graph.node_link_graph(G_data)\n",
    "    id_map = json.load(open(prefix + \"-id_map.json\"))\n",
    "    id_map = {str(k): int(v) for k,v in id_map.items()}\n",
    "    feats = np.load(prefix + \"-feats.npy\")\n",
    "    feats = np.vstack([feats, np.zeros((feats.shape[1],), dtype=feats.dtype)])\n",
    "    n = len(id_map)\n",
    "    adj = (n+1)*np.ones((n+1, MAX_DEGREE), dtype=np.int32)\n",
    "    deg = np.zeros((n,), dtype=np.int32)\n",
    "    for node in G.nodes():\n",
    "        u = id_map[str(node)]\n",
    "        neigh = [id_map[str(v)] for v in G.neighbors(node)]\n",
    "        deg[u] = len(neigh)\n",
    "        if len(neigh)==0: continue\n",
    "        if len(neigh)>MAX_DEGREE: neigh = np.random.choice(neigh, MAX_DEGREE, replace=False)\n",
    "        elif len(neigh)<MAX_DEGREE: neigh = np.random.choice(neigh, MAX_DEGREE, replace=True)\n",
    "        adj[u,:] = np.array(neigh, dtype=np.int32)\n",
    "    return G, feats, id_map, adj, deg\n",
    "\n",
    "def load_edge_split(prefix, split):\n",
    "    edges = np.load(f\"{prefix}-{split}_edges.npy\").astype(np.int32)\n",
    "    efeat = np.load(f\"{prefix}-{split}_edge_feats.npy\").astype(np.float32)\n",
    "    labels= np.load(f\"{prefix}-{split}_edge_labels.npy\").astype(np.int32)\n",
    "    cpb   = np.load(f\"{prefix}-{split}_cp_bucket.npy\").astype(np.int32)\n",
    "    txids = np.load(f\"{prefix}-{split}_txids.npy\")\n",
    "    return edges, efeat, labels, cpb, txids\n",
    "\n",
    "class EdgeLoader:\n",
    "    def __init__(self, prefix):\n",
    "        self.train = load_edge_split(prefix, \"train\")\n",
    "        self.valid = load_edge_split(prefix, \"valid\")\n",
    "        self.test  = load_edge_split(prefix, \"test\")\n",
    "        self.F = self.train[1].shape[1]\n",
    "        y = self.train[2]; self.pos_rate = (y==1).mean() if len(y) else 0.0\n",
    "    def iter_batches(self, batch_size, split=\"train\", shuffle=True):\n",
    "        edges, efeat, labels, cpb, _ = getattr(self, split)\n",
    "        idx = np.arange(len(labels))\n",
    "        if shuffle: np.random.shuffle(idx)\n",
    "        for i in range(0, len(idx), batch_size):\n",
    "            sel = idx[i:i+batch_size]\n",
    "            yield edges[sel,0], edges[sel,1], efeat[sel], labels[sel], cpb[sel]\n",
    "\n",
    "class EdgeSupGraphSAGE:\n",
    "    def __init__(self, feats, adj, deg, aggregator=\"mean\",\n",
    "                 dim_1=128, dim_2=128, samples_1=25, samples_2=10,\n",
    "                 dropout=0.0, lr=1e-3, cp_n_buckets=4096, cp_emb_dim=8, concat=True):\n",
    "        self.feats_np, self.adj_np, self.deg_np = feats, adj, deg\n",
    "        self.dropout_rate, self.lr, self.concat = dropout, lr, concat\n",
    "\n",
    "        self.batch_size_ph = tf1.placeholder(tf.int32, shape=(), name=\"batch_size\")\n",
    "        self.batch_u = tf1.placeholder(tf.int32, shape=[None], name=\"batch_src\")\n",
    "        self.batch_v = tf1.placeholder(tf.int32, shape=[None], name=\"batch_dst\")\n",
    "        self.edge_feats = tf1.placeholder(tf.float32, shape=[None, feats.shape[1]*0+25], name=\"edge_feats\")  # dim set at feed\n",
    "        self.cp_idx     = tf1.placeholder(tf.int32,   shape=[None], name=\"cp_bucket\")\n",
    "        self.labels     = tf1.placeholder(tf.float32, shape=[None], name=\"labels\")\n",
    "        self.pos_weight = tf1.placeholder(tf.float32, shape=(), name=\"pos_weight\")\n",
    "        self.dropout_ph = tf1.placeholder_with_default(0.0, shape=(), name=\"dropout\")\n",
    "\n",
    "        self.adj   = tf1.Variable(self.adj_np, trainable=False, dtype=tf.int32, name=\"adj_info\")\n",
    "        self.feats = tf1.Variable(tf.constant(self.feats_np, dtype=tf.float32), trainable=False, name=\"node_feats\")\n",
    "        self.degrees = tf.constant(self.deg_np, dtype=tf.int32)\n",
    "\n",
    "        sampler = UniformNeighborSampler(self.adj)\n",
    "        agg_cls = {\"mean\": MeanAggregator, \"gcn\": GCNAggregator,\n",
    "                   \"maxpool\": MaxPoolingAggregator, \"meanpool\": MeanPoolingAggregator, \"seq\": SeqAggregator}[aggregator]\n",
    "\n",
    "        SAGEInfo = __import__('graphsage.models', fromlist=['SAGEInfo']).models.SAGEInfo\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, samples_1, dim_1),\n",
    "                       SAGEInfo(\"node\", sampler, samples_2, dim_2)]\n",
    "        num_samples = [samples_1, samples_2]\n",
    "        dims = [self.feats_np.shape[1], dim_1, dim_2]\n",
    "\n",
    "        def sample(inputs, layer_infos, batch_size):\n",
    "            samples = [inputs]; support_size = 1; support_sizes = [support_size]\n",
    "            for k in range(len(layer_infos)):\n",
    "                t = len(layer_infos) - k - 1\n",
    "                support_size *= layer_infos[t].num_samples\n",
    "                node = layer_infos[t].neigh_sampler((samples[k], layer_infos[t].num_samples))\n",
    "                samples.append(tf.reshape(node, [support_size * batch_size,]))\n",
    "                support_sizes.append(support_size)\n",
    "            return samples, support_sizes\n",
    "\n",
    "        def aggregate(samples, input_features, dims, num_samples, support_sizes, batch_size, aggregators=None, concat=True):\n",
    "            hidden = [tf.nn.embedding_lookup(input_features, node_samples) for node_samples in samples]\n",
    "            new_agg = aggregators is None\n",
    "            if new_agg: aggregators = []\n",
    "            for layer in range(len(num_samples)):\n",
    "                if new_agg:\n",
    "                    dim_mult = 2 if concat and (layer != 0) else 1\n",
    "                    act = (lambda x: x) if (layer == len(num_samples)-1) else tf.nn.relu\n",
    "                    aggregator = agg_cls(dim_mult*dims[layer], dims[layer+1], dropout=self.dropout_ph, concat=concat)\n",
    "                    aggregators.append(aggregator)\n",
    "                else:\n",
    "                    aggregator = aggregators[layer]\n",
    "                next_hidden = []\n",
    "                for hop in range(len(num_samples) - layer):\n",
    "                    dim_mult = 2 if concat and (layer != 0) else 1\n",
    "                    neigh_dims = [batch_size * support_sizes[hop],\n",
    "                                  num_samples[len(num_samples)-hop-1],\n",
    "                                  dim_mult*dims[layer]]\n",
    "                    h = aggregator((hidden[hop], tf.reshape(hidden[hop+1], neigh_dims)))\n",
    "                    next_hidden.append(h)\n",
    "                hidden = next_hidden\n",
    "            return hidden[0], aggregators\n",
    "\n",
    "        su, supp_u = sample(self.batch_u, layer_infos, self.batch_size_ph)\n",
    "        sv, supp_v = sample(self.batch_v, layer_infos, self.batch_size_ph)\n",
    "        h_u, aggs = aggregate(su, self.feats, dims, num_samples, supp_u, self.batch_size_ph, concat=self.concat)\n",
    "        h_v, _    = aggregate(sv, self.feats, dims, num_samples, supp_v, self.batch_size_ph, aggregators=aggs, concat=self.concat)\n",
    "\n",
    "        h_u = tf.nn.l2_normalize(h_u, axis=1)\n",
    "        h_v = tf.nn.l2_normalize(h_v, axis=1)\n",
    "\n",
    "        self.cp_emb = tf1.get_variable(\"cp_emb\", shape=[int(json.load(open(OUT_PREFIX+\"-meta.json\")).get(\"cp_n_buckets\",4096)), CP_EMB_DIM],\n",
    "                                       initializer=xavier_init)\n",
    "        emb_cp = tf.nn.embedding_lookup(self.cp_emb, self.cp_idx)\n",
    "\n",
    "        edge_input = tf.concat([h_u, h_v, self.edge_feats, emb_cp], axis=1)\n",
    "        in_dim = int(edge_input.shape[1])\n",
    "\n",
    "        W1 = tf1.get_variable(\"edge_fc1\", shape=[in_dim, 128], initializer=xavier_init)\n",
    "        b1 = tf1.get_variable(\"edge_b1\", shape=[128], initializer=zeros_init)\n",
    "        W2 = tf1.get_variable(\"edge_fc2\", shape=[128, 1], initializer=xavier_init)\n",
    "        b2 = tf1.get_variable(\"edge_b2\", shape=[1], initializer=zeros_init)\n",
    "\n",
    "        z1 = tf.nn.relu(tf.matmul(edge_input, W1) + b1)\n",
    "        z1 = tf.nn.dropout(z1, keep_prob=1.0 - self.dropout_ph)\n",
    "        logits = tf.squeeze(tf.matmul(z1, W2) + b2, axis=1)\n",
    "        self.probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(\n",
    "            targets=self.labels, logits=logits, pos_weight=self.pos_weight))\n",
    "        self.opt = tf1.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "    def train_epoch(self, sess, loader, pos_weight, dropout):\n",
    "        tot, n = 0.0, 0\n",
    "        for (u,v,e,y,cp) in loader.iter_batches(BATCH_SIZE, \"train\", True):\n",
    "            feed = { self.batch_size_ph: len(y), self.batch_u: u, self.batch_v: v,\n",
    "                     self.edge_feats: e, self.labels: y.astype(np.float32),\n",
    "                     self.cp_idx: cp, self.pos_weight: pos_weight, self.dropout_ph: dropout }\n",
    "            _, L = sess.run([self.opt, self.loss], feed_dict=feed)\n",
    "            tot += L*len(y); n += len(y)\n",
    "        return tot/max(n,1)\n",
    "\n",
    "    def predict_all(self, sess, loader, split=\"valid\"):\n",
    "        yt, pt = [], []\n",
    "        for (u,v,e,y,cp) in loader.iter_batches(BATCH_SIZE, split, False):\n",
    "            feed = { self.batch_size_ph: len(y), self.batch_u: u, self.batch_v: v,\n",
    "                     self.edge_feats: e, self.cp_idx: cp, self.dropout_ph: 0.0 }\n",
    "            p = sess.run(self.probs, feed_dict=feed)\n",
    "            yt.append(y); pt.append(p)\n",
    "        return np.concatenate(yt), np.concatenate(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uB2qtVL6asDZ"
   },
   "outputs": [],
   "source": [
    "G, feats, id_map, adj, deg = load_graph_base(OUT_PREFIX)\n",
    "loader = EdgeLoader(OUT_PREFIX)\n",
    "print(\"Edge feat dim:\", loader.F, \"| train pos rate:\", loader.pos_rate)\n",
    "\n",
    "y_tr = loader.train[2]\n",
    "pos_w = float((len(y_tr) - y_tr.sum()) / max(y_tr.sum(), 1))\n",
    "\n",
    "tf1.reset_default_graph()\n",
    "model = EdgeSupGraphSAGE(feats, adj, deg, aggregator=AGGREGATOR,\n",
    "                         dim_1=DIM_1, dim_2=DIM_2, samples_1=SAMPLES_1, samples_2=SAMPLES_2,\n",
    "                         dropout=DROPOUT, lr=LR, cp_n_buckets=int(json.load(open(OUT_PREFIX+\"-meta.json\")).get(\"cp_n_buckets\",4096)),\n",
    "                         cp_emb_dim=CP_EMB_DIM, concat=True)\n",
    "\n",
    "config = tf1.ConfigProto(); config.gpu_options.allow_growth = True\n",
    "sess = tf1.Session(config=config); sess.run(tf1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qBWNncUavo8"
   },
   "outputs": [],
   "source": [
    "# for ep in range(1, EPOCHS+1):\n",
    "#     t0 = time.time()\n",
    "#     loss_tr = model.train_epoch(sess, loader, pos_weight=pos_w, dropout=DROPOUT)\n",
    "#     yv, pv = model.predict_all(sess, loader, \"valid\")\n",
    "#     aucpr = average_precision_score(yv, pv) if len(yv)>0 else np.nan\n",
    "#     auc   = roc_auc_score(yv, pv) if len(np.unique(yv))>1 else np.nan\n",
    "#     f1    = f1_score(yv, (pv>=0.5).astype(int), zero_division=0) if len(yv)>0 else np.nan\n",
    "#     print(f\"[Epoch {ep}] loss={loss_tr:.5f} | val AUCPR={aucpr:.4f} ROC-AUC={auc:.4f} F1@0.5={f1:.4f} | {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUkUYtMbax1i"
   },
   "outputs": [],
   "source": [
    "# yt, pt = model.predict_all(sess, loader, \"test\")\n",
    "# aucpr_t = average_precision_score(yt, pt) if len(yt)>0 else np.nan\n",
    "# auc_t   = roc_auc_score(yt, pt) if len(np.unique(yt))>1 else np.nan\n",
    "# f1_t    = f1_score(yt, (pt>=0.5).astype(int), zero_division=0) if len(yt)>0 else np.nan\n",
    "# print(f\"[TEST] AUCPR={aucpr_t:.4f} ROC-AUC={auc_t:.4f} F1@0.5={f1_t:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0ydQ9GIoLIM"
   },
   "outputs": [],
   "source": [
    "RESULTS_ROOT = os.path.join(DATA_DIR, \"results\")\n",
    "tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "run_id = datetime.now(tz).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "RUN_DIR = os.path.join(RESULTS_ROOT, run_id)\n",
    "CKPT_DIR = os.path.join(RUN_DIR, \"ckpt\")\n",
    "PLOTS_DIR = os.path.join(RUN_DIR, \"plots\")\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "latest_link = os.path.join(RESULTS_ROOT, \"latest\")\n",
    "try:\n",
    "    if os.path.islink(latest_link) or os.path.exists(latest_link):\n",
    "        if os.path.islink(latest_link):\n",
    "            os.unlink(latest_link)\n",
    "        else:\n",
    "            shutil.rmtree(latest_link)\n",
    "    os.symlink(RUN_DIR, latest_link)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "exp_config = {\n",
    "    \"run_id\": run_id,\n",
    "    \"data_dir\": DATA_DIR,\n",
    "    \"proc_dir\": PROC_DIR,\n",
    "    \"train_prefix\": OUT_PREFIX,\n",
    "    \"epochs\": int(EPOCHS),\n",
    "    \"dropout\": float(DROPOUT),\n",
    "    \"pos_weight\": float(pos_w) if 'pos_w' in globals() else None,\n",
    "}\n",
    "with open(os.path.join(RUN_DIR, \"config.json\"), \"w\") as f:\n",
    "    json.dump(exp_config, f, indent=2)\n",
    "\n",
    "def save_pr_roc(y_true, y_score, split_name):\n",
    "    if len(y_true) == 0 or np.unique(y_true).size < 2:\n",
    "        return\n",
    "    p, r, _ = precision_recall_curve(y_true, y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "    plt.figure()\n",
    "    plt.step(r, p, where=\"post\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR — {split_name} (AP={ap:.4f})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, f\"{split_name}_pr_curve.png\"), dpi=150)\n",
    "    plt.close()\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {split_name} (AUC={auc:.4f})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, f\"{split_name}_roc_curve.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def maybe_get_txids(split):\n",
    "    try:\n",
    "        return loader.get_txids(split)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return getattr(loader, f\"{split}_txids\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def threshold_metrics(y_true, y_score, thr=0.5):\n",
    "    if len(y_true) == 0:\n",
    "        return {\"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan, \"accuracy\": np.nan}\n",
    "    y_hat = (y_score >= thr).astype(int)\n",
    "    return {\n",
    "        \"precision\": precision_score(y_true, y_hat, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_hat, zero_division=0),\n",
    "        \"f1\":        f1_score(y_true, y_hat, zero_division=0),\n",
    "        \"accuracy\":  accuracy_score(y_true, y_hat),\n",
    "    }\n",
    "\n",
    "history = []\n",
    "best_val_aucpr = -np.inf\n",
    "saver = tf.compat.v1.train.Saver(max_to_keep=3)\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    loss_tr = model.train_epoch(sess, loader, pos_weight=pos_w, dropout=DROPOUT)\n",
    "    yv, pv = model.predict_all(sess, loader, \"valid\")\n",
    "    aucpr = average_precision_score(yv, pv) if len(yv) > 0 else np.nan\n",
    "    auc   = roc_auc_score(yv, pv) if np.unique(yv).size > 1 else np.nan\n",
    "    m_val = threshold_metrics(yv, pv, thr=0.5)\n",
    "    dur = time.time() - t0\n",
    "    print(f\"[Epoch {ep}] loss={loss_tr:.5f} | val AUCPR={aucpr:.4f} ROC-AUC={auc:.4f} \"\n",
    "          f\"P={m_val['precision']:.4f} R={m_val['recall']:.4f} F1@0.5={m_val['f1']:.4f} Acc@0.5={m_val['accuracy']:.4f} | {dur:.1f}s\")\n",
    "    history.append({\n",
    "        \"epoch\": ep,\n",
    "        \"train_loss\": float(loss_tr),\n",
    "        \"val_aucpr\": float(aucpr) if not np.isnan(aucpr) else None,\n",
    "        \"val_rocauc\": float(auc)   if not np.isnan(auc)   else None,\n",
    "        \"val_precision_0.5\": float(m_val[\"precision\"]) if not np.isnan(m_val[\"precision\"]) else None,\n",
    "        \"val_recall_0.5\":    float(m_val[\"recall\"])    if not np.isnan(m_val[\"recall\"])    else None,\n",
    "        \"val_f1_0.5\":        float(m_val[\"f1\"])        if not np.isnan(m_val[\"f1\"])        else None,\n",
    "        \"val_accuracy_0.5\":  float(m_val[\"accuracy\"])  if not np.isnan(m_val[\"accuracy\"])  else None,\n",
    "        \"time_sec\": float(dur),\n",
    "    })\n",
    "    pd.DataFrame(history).to_csv(os.path.join(RUN_DIR, \"metrics_history.csv\"), index=False)\n",
    "    with open(os.path.join(RUN_DIR, \"metrics_history.json\"), \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    if not np.isnan(aucpr) and aucpr > best_val_aucpr:\n",
    "        best_val_aucpr = aucpr\n",
    "        ckpt_path = os.path.join(CKPT_DIR, f\"best_val_aucpr_ep{ep:03d}.ckpt\")\n",
    "        saver.save(sess, ckpt_path)\n",
    "        with open(os.path.join(RUN_DIR, \"best_val.json\"), \"w\") as f:\n",
    "            json.dump({\"best_epoch\": ep, \"best_val_aucpr\": float(best_val_aucpr), \"ckpt\": ckpt_path}, f, indent=2)\n",
    "\n",
    "if len(yv) > 0:\n",
    "    save_pr_roc(yv, pv, \"valid\")\n",
    "    val_df = pd.DataFrame({\"y_true\": yv.astype(int), \"y_score\": pv.astype(float)})\n",
    "    txv = maybe_get_txids(\"valid\")\n",
    "    if txv is not None and len(txv) == len(val_df):\n",
    "        val_df.insert(0, \"tx_id\", txv)\n",
    "    val_df.to_csv(os.path.join(RUN_DIR, \"val_predictions.csv\"), index=False)\n",
    "    if np.unique(yv).size > 1:\n",
    "        cm_v = confusion_matrix(yv, (pv >= 0.5).astype(int))\n",
    "        with open(os.path.join(RUN_DIR, \"val_confusion_matrix.json\"), \"w\") as f:\n",
    "            json.dump({\"labels\":[0,1], \"matrix\": cm_v.tolist()}, f, indent=2)\n",
    "\n",
    "yt, pt = model.predict_all(sess, loader, \"test\")\n",
    "aucpr_t = average_precision_score(yt, pt) if len(yt) > 0 else np.nan\n",
    "auc_t   = roc_auc_score(yt, pt) if np.unique(yt).size > 1 else np.nan\n",
    "m_test  = threshold_metrics(yt, pt, thr=0.5)\n",
    "print(f\"[TEST] AUCPR={aucpr_t:.4f} ROC-AUC={auc_t:.4f} \"\n",
    "      f\"P={m_test['precision']:.4f} R={m_test['recall']:.4f} F1@0.5={m_test['f1']:.4f} Acc@0.5={m_test['accuracy']:.4f}\")\n",
    "\n",
    "test_metrics = {\n",
    "    \"test_aucpr\":  float(aucpr_t) if not np.isnan(aucpr_t) else None,\n",
    "    \"test_rocauc\": float(auc_t)   if not np.isnan(auc_t)   else None,\n",
    "    \"test_precision_0.5\": float(m_test[\"precision\"]) if not np.isnan(m_test[\"precision\"]) else None,\n",
    "    \"test_recall_0.5\":    float(m_test[\"recall\"])    if not np.isnan(m_test[\"recall\"])    else None,\n",
    "    \"test_f1_0.5\":        float(m_test[\"f1\"])        if not np.isnan(m_test[\"f1\"])        else None,\n",
    "    \"test_accuracy_0.5\":  float(m_test[\"accuracy\"])  if not np.isnan(m_test[\"accuracy\"])  else None,\n",
    "}\n",
    "with open(os.path.join(RUN_DIR, \"test_metrics.json\"), \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "if len(yt) > 0:\n",
    "    save_pr_roc(yt, pt, \"test\")\n",
    "    test_df = pd.DataFrame({\"y_true\": yt.astype(int), \"y_score\": pt.astype(float)})\n",
    "    txt = maybe_get_txids(\"test\")\n",
    "    if txt is not None and len(txt) == len(test_df):\n",
    "        test_df.insert(0, \"tx_id\", txt)\n",
    "    test_df.to_csv(os.path.join(RUN_DIR, \"test_predictions.csv\"), index=False)\n",
    "    if np.unique(yt).size > 1:\n",
    "        cm_t = confusion_matrix(yt, (pt >= 0.5).astype(int))\n",
    "        with open(os.path.join(RUN_DIR, \"test_confusion_matrix.json\"), \"w\") as f:\n",
    "            json.dump({\"labels\":[0,1], \"matrix\": cm_t.tolist()}, f, indent=2)\n",
    "\n",
    "if len(yt) > 0:\n",
    "    k = min(200, len(yt))\n",
    "    topk_idx = np.argsort(-pt)[:k]\n",
    "    topk = {\"index\": topk_idx.tolist(),\n",
    "            \"y_true\": yt[topk_idx].astype(int).tolist(),\n",
    "            \"y_score\": pt[topk_idx].astype(float).tolist()}\n",
    "    if 'txt' in locals() and txt is not None and len(txt) == len(yt):\n",
    "        topk[\"tx_id\"] = [str(txt[i]) for i in topk_idx]\n",
    "    with open(os.path.join(RUN_DIR, \"test_topk.json\"), \"w\") as f:\n",
    "        json.dump(topk, f, indent=2)\n",
    "\n",
    "print(f\"\\nresultados salvos em:\\n{RUN_DIR}\\n(atalho: {latest_link if os.path.exists(latest_link) else RUN_DIR})\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMiShUx8K+DqCmLxho4aO/J",
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
